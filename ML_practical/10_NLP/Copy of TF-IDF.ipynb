{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCQPScy5Oz4OdFoMxWp9Oq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GbbK3lyAvOVw","executionInfo":{"status":"ok","timestamp":1688103596769,"user_tz":-330,"elapsed":6,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"042b0974-6fed-4182-dd9d-258ed1f64c61"},"outputs":[{"output_type":"stream","name":"stdout","text":["['It is going to rain today.', 'Today I am not going outside.', 'I am going to watch the season premiere.']\n"]},{"output_type":"execute_result","data":{"text/plain":["['It is going to rain today.',\n"," 'Today I am not going outside.',\n"," 'I am going to watch the season premiere.']"]},"metadata":{},"execution_count":3}],"source":["Document1 = 'It is going to rain today.'\n","Document2= 'Today I am not going outside.'\n","\n","Document3= 'I am going to watch the season premiere.'\n","\n","\n","Doc = [Document1, Document2, Document3]\n","\n","print (Doc)\n","\n","['It is going to rain today.', 'Today I am not going outside.', 'I am going to watch the season premiere.']\n","\n"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","\n","analyze = vectorizer.build_analyzer()\n","\n","print('Document 1', analyze (Document1))\n","print('Document 2', analyze (Document2))\n","print('Document 3', analyze (Document3))\n","\n","x = vectorizer.fit_transform(Doc)\n","print(vectorizer.get_feature_names_out())\n","print('Document transform',x.toarray())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wu-gQLaL0M5o","executionInfo":{"status":"ok","timestamp":1688103757269,"user_tz":-330,"elapsed":472,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"4e9fd670-ed7d-4aef-fcb4-cb242c03e0d4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1 ['it', 'is', 'going', 'to', 'rain', 'today']\n","Document 2 ['today', 'am', 'not', 'going', 'outside']\n","Document 3 ['am', 'going', 'to', 'watch', 'the', 'season', 'premiere']\n","['am' 'going' 'is' 'it' 'not' 'outside' 'premiere' 'rain' 'season' 'the'\n"," 'to' 'today' 'watch']\n","Document transform [[0.         0.27824521 0.4711101  0.4711101  0.         0.\n","  0.         0.4711101  0.         0.         0.35829137 0.35829137\n","  0.        ]\n"," [0.40619178 0.31544415 0.         0.         0.53409337 0.53409337\n","  0.         0.         0.         0.         0.         0.40619178\n","  0.        ]\n"," [0.32412354 0.25171084 0.         0.         0.         0.\n","  0.4261835  0.         0.4261835  0.4261835  0.32412354 0.\n","  0.4261835 ]]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"metadata":{"id":"Rzi7DLXS12sc","executionInfo":{"status":"ok","timestamp":1688104072138,"user_tz":-330,"elapsed":3,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import sklearn as sk\n","import math\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import re\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","first_sentence = \"data science is the hotest job of the 21st century\"\n","second_sentence = \"machine learning is the key for data science\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhRamLOY1_8h","executionInfo":{"status":"ok","timestamp":1688104509095,"user_tz":-330,"elapsed":3291,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"c761110d-4104-4bd9-c67c-052e0f103fb9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["#split so each word have their own string\n","first_sentence = first_sentence.split(\" \")\n","second_sentence = second_sentence.split(\" \")#join the\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","first_sentence = [w for w in first_sentence if not w in stop_words]\n","second_sentence = [w for w in second_sentence if not w in stop_words]\n","print(first_sentence)\n","print(second_sentence)\n","\n","\n","total= set(first_sentence).union(set(second_sentence))\n","\n","print(total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9H3eYyw23qid","executionInfo":{"status":"ok","timestamp":1688104535798,"user_tz":-330,"elapsed":5,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"034013bb-a679-49a9-cf78-8b655844a517"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['data', 'science', 'hotest', 'job', '21st', 'century']\n","['machine', 'learning', 'key', 'data', 'science']\n","{'science', 'hotest', 'century', 'key', 'machine', 'job', 'data', 'learning', '21st'}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["wordDictA = dict.fromkeys(total, 0)\n","wordDictB = dict.fromkeys(total, 0)\n","for word in first_sentence:\n","    wordDictA[word]+=1\n","\n","for word in second_sentence:\n","    wordDictB[word]+=1\n","\n","print(wordDictA)\n","print(wordDictB)\n","pd.DataFrame([wordDictA, wordDictB])\n","from numpy import log10\n","def computeTF(wordDict, doc):\n","    tfDict = {}\n","    corpusCount = len(doc)\n","    for word, count in wordDict.items():\n","        tfDict[word] = count/float(corpusCount)\n","    return(tfDict)\n","\n","#running our sentences through the tf function:\n","tfFirst = computeTF(wordDictA, first_sentence)\n","tfSecond = computeTF(wordDictB, second_sentence)\n","\n","#Converting to dataframe for visualization\n","tf = pd.DataFrame([tfFirst, tfSecond])\n","tf = 1 + log10(tf)\n","tf\n","def computeIDF(docList):\n","    idfDict = {}\n","    N = len(docList)\n","\n","    idfDict = dict.fromkeys(docList[0].keys(), 0)\n","    for word, val in idfDict.items():\n","        #print(word, val)\n","        idfDict[word] = math.log10(N / (float(val) + 1 ))\n","\n","    return(idfDict)\n","\n","#inputing our sentences in the log file\n","idfs = computeIDF([wordDictA, wordDictB])\n","idfs\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZW7vbQn3z7n","executionInfo":{"status":"ok","timestamp":1688104553929,"user_tz":-330,"elapsed":483,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"17e85af6-e391-493f-c696-53422249372e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'science': 1, 'hotest': 1, 'century': 1, 'key': 0, 'machine': 0, 'job': 1, 'data': 1, 'learning': 0, '21st': 1}\n","{'science': 1, 'hotest': 0, 'century': 0, 'key': 1, 'machine': 1, 'job': 0, 'data': 1, 'learning': 1, '21st': 0}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: divide by zero encountered in log10\n","  result = func(self.values, **kwargs)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'science': 0.3010299956639812,\n"," 'hotest': 0.3010299956639812,\n"," 'century': 0.3010299956639812,\n"," 'key': 0.3010299956639812,\n"," 'machine': 0.3010299956639812,\n"," 'job': 0.3010299956639812,\n"," 'data': 0.3010299956639812,\n"," 'learning': 0.3010299956639812,\n"," '21st': 0.3010299956639812}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def computeTFIDF(tfBow, idfs):\n","    tfidf = {}\n","    for word, val in tfBow.items():\n","        tfidf[word] = val*idfs[word]\n","    return(tfidf)\n","#running our two sentences through the IDF:\n","idfFirst = computeTFIDF(tfFirst, idfs)\n","idfSecond = computeTFIDF(tfSecond, idfs)\n","#putting it in a dataframe\n","idf= pd.DataFrame([idfFirst, idfSecond])\n","print(idf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0L1F8lg4Gmf","executionInfo":{"status":"ok","timestamp":1688104630096,"user_tz":-330,"elapsed":435,"user":{"displayName":"Manthan_ Mehar_DBDA","userId":"10430341665446730671"}},"outputId":"8577a95d-efda-42ea-8ab1-9be38c31fe3d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["    science    hotest   century       key   machine       job      data  \\\n","0  0.050172  0.050172  0.050172  0.000000  0.000000  0.050172  0.050172   \n","1  0.060206  0.000000  0.000000  0.060206  0.060206  0.000000  0.060206   \n","\n","   learning      21st  \n","0  0.000000  0.050172  \n","1  0.060206  0.000000  \n"]}]}]}